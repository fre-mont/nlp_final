{"cells":[{"cell_type":"markdown","metadata":{"id":"q1pSrdInp-H5"},"source":["### TOKENIZER\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd \n","import argparse\n","import pandas as pd\n","import os\n","import sentencepiece as spm\n","import codecs\n","import pickle\n","\n","train_df = pd.read_csv(f\"data/train.csv\",encoding='utf-8')\n","\n","train_dialect = train_df['dialect_form'].values.tolist()\n","train_standard = train_df['standard_form'].values.tolist()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# 모든 요소를 문자열로 변환\n","train_dialect_str = list(map(str, train_dialect))\n","train_standard_str = list(map(str, train_standard))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["with codecs.open(f\"tokenizer/bpe.train\",'w','utf8') as fout:\n","    fout.write(\"\\n\".join(train_dialect_str + train_standard_str))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def train_bpe(fpath, vocab_size=4000):\n","    dir = os.path.dirname(fpath)\n","    train = f'--input={fpath} \\\n","              --normalization_rule_name=identity \\\n","              --model_prefix={dir}/bpe_{vocab_size} \\\n","              --character_coverage=1.0 \\\n","              --vocab_size={vocab_size} \\\n","              --model_type=bpe \\\n","              --unk_id=0 \\\n","              --bos_id=1 \\\n","              --eos_id=2 \\\n","              --pad_id=3 \\\n","              --unk_piece=<unk> \\\n","              --bos_piece=<bos> \\\n","              --eos_piece=<eos> \\\n","              --pad_piece=<pad>'\n","    spm.SentencePieceTrainer.Train(train)\n","\n","\n","train_bpe(f'tokenizer/bpe.train',vocab_size=8000)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["sp = spm.SentencePieceProcessor()\n","sp.Load(f'tokenizer/bpe_8000.model')\n","\n","# Save Vocab\n","vocab = {}\n","for id_ in range(sp.get_piece_size()):\n","    token = sp.id_to_piece(id_)\n","    vocab[id_] = token\n","\n","with open(f'tokenizer/vocab_dict_8000.pickle','wb') as f:\n","    pickle.dump(vocab,f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZb9zjip5tfG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN/M1s9tCxiVEthWwTrkza+","gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
