{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedTokenizerFast, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "class CustomSPTokenizer(PreTrainedTokenizerFast):\n",
    "    def __init__(self, tokenizer_path):\n",
    "        super().__init__(tokenizer_file=tokenizer_path)\n",
    "        self._tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self._tokenizer.encode(text).tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self._tokenizer.token_to_id(token)\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self._tokenizer.id_to_token(index)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return self._tokenizer.decode(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep + [self.sep_token_id]\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<cls>\")\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<sep>\")\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<pad>\")\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self):\n",
    "        return self._tokenizer.token_to_id(\"<unk>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model/bartbase_model2'\n",
    "tokenizer_path = \"data/bpe_tokenizer.json\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "tokenizer = CustomSPTokenizer(tokenizer_path)\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.pad_token = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반 갑 습니다\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def translate(text):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(model.device) for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=100,\n",
    "    )\n",
    "    \n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "    \n",
    "    \n",
    "text = \"반갑수다\"\n",
    "translated_text = translate(text)\n",
    "\n",
    "print(translated_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
